[auth]
username = 61423349819 ; Required: Login username/mobile number.
password = Falcon66! ; Required: Login password.

[scraper]
url_list_path = urls.txt ; Required: Path to the list of target URLs.
max_concurrent_requests = 1 ; Number of parallel tasks.
min_request_delay = 0.6 ; Minimum seconds between requests.
max_request_delay = 1.4 ; Maximum seconds between requests.

[output]
enable_csv_output = true
csv_output_path = data/bonuses.csv
enable_db_output = true
db_connection_string = sqlite:///data/bonuses.db

[logging]
log_level = DEBUG ; Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
log_file_path = log/log.log# config.py 
import configparser
import sys
import os

def get_config(path: str = "config.ini") -> configparser.ConfigParser:
    if not os.path.exists(path):
        print(f"FATAL ERROR: Configuration file not found at '{path}'")
        sys.exit(1)
    
    config = configparser.ConfigParser(inline_comment_prefixes=(';', '#'))
    config.read(path)
    
    required = {
        "auth": ["username", "password"],
        "scraper": ["url_list_path"],
        "output": ["db_connection_string"],
        "logging": ["log_file_path"],
    }
    for section, keys in required.items():
        if not config.has_section(section):
            print(f"FATAL ERROR: Missing required section '[{section}]' in '{path}'")
            sys.exit(1)
        for key in keys:
            if not config.has_option(section, key):
                print(f"FATAL ERROR: Missing required key '{key}' in section '[{section}]'")
                sys.exit(1)
    return configimport datetime
from sqlalchemy import Column, Integer, String, Float, Boolean, DateTime
from sqlalchemy.orm import declarative_base
from pydantic import BaseModel

class AuthData(BaseModel):
    merchant_id: str
    merchant_name: str
    access_id: str
    token: str
    api_url: str

Base = declarative_base()

class Bonus(Base):
    __tablename__ = 'bonuses'
    db_id = Column(Integer, primary_key=True, autoincrement=True)
    url = Column(String)
    merchant_name = Column(String)
    id = Column(String, index=True)
    name = Column(String)
    amount = Column(Float)
    rollover = Column(Float)
    bonus_fixed = Column(Float)
    min_withdraw = Column(Float)
    max_withdraw = Column(Float)
    withdraw_to_bonus_ratio = Column(Float, nullable=True)
    min_topup = Column(Float)
    max_topup = Column(Float)
    transaction_type = Column(String)
    balance = Column(String)
    bonus = Column(String)
    bonus_random = Column(String)
    reset = Column(String)
    refer_link = Column(String)
    is_auto_claim = Column(Boolean, default=False)
    is_vip_only = Column(Boolean, default=False)
    has_loss_requirement = Column(Boolean, default=False)
    has_topup_requirement = Column(Boolean, default=False)
    loss_req_percent = Column(Float, nullable=True)
    loss_req_amount = Column(Float, nullable=True)
    topup_req_amount = Column(Float, nullable=True)
    claim_type = Column(String, nullable=True)
    raw_claim_config = Column(String)
    raw_claim_condition = Column(String)
    created_at = Column(DateTime, default=datetime.datetime.utcnow)# auth.py 
import configparser, logging, re, time, asyncio
from typing import Optional, Deque
from pydantic import ValidationError
import aiohttp
from models import AuthData

async def get_auth(url: str, config: configparser.ConfigParser, logger: logging.Logger, session: aiohttp.ClientSession, request_tracker: Deque[float]) -> Optional[AuthData]:
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    request_tracker.append(time.time())
    
    try:
        async with session.get(url, headers=headers, proxy=None, timeout=15, ssl=False) as response:
            response.raise_for_status()
            html = await response.text()
            if not html:
                logger.warning("auth_html_empty", extra={"url": url})
                return None
            match = re.search(r'var MERCHANTID = (\d+);', html, re.IGNORECASE)
            if not match:
                logger.warning("auth_merch_id_fail", extra={"url": url})
                return None
            merchant_id = match.group(1)
            merchant_name_match = re.search(r'var MERCHANTNAME = ["\'](.*?)["\'];', html, re.IGNORECASE)
            merchant_name = merchant_name_match.group(1) if merchant_name_match else ""
    except Exception as e:
        logger.error("auth_html_fetch_fail", extra={"url": url, "err": str(e)})
        return None

    api_url = f"{url}/api/v1/index.php"
    payload = {"module": "/users/login", "mobile": config.get('auth', 'username'), "password": config.get('auth', 'password'), "merchantId": merchant_id}
    
    try:
        async with session.post(api_url, data=payload, headers=headers, proxy=None, timeout=15, ssl=False) as response:
            response.raise_for_status()
            res_json = await response.json()
            if res_json.get("status") != "SUCCESS":
                logger.warning("auth_api_status_fail", extra={"url": api_url, "response": res_json})
                return None
            auth_payload = {
                "merchant_id": merchant_id, "merchant_name": merchant_name,
                "access_id": res_json.get("data", {}).get("id"),
                "token": res_json.get("data", {}).get("token"),
                "api_url": api_url
            }
            return AuthData.model_validate(auth_payload)
    except Exception as e:
        logger.error("auth_api_request_fail", extra={"url": api_url, "err": str(e)})
        return Noneimport logging
import os
import json
from logging.handlers import RotatingFileHandler

class DetailFormatter(logging.Formatter):
    def format(self, record):
        log_string = super().format(record)
        extra_items = {k:v for k,v in record.__dict__.items() if k not in logging.LogRecord.__dict__ and k != 'args'}
        if extra_items:
            details_str = json.dumps(extra_items)
            log_string += f" -- Details: {details_str}"
        return log_string

def setup_logger(config):
    log_level = config.get('logging', 'log_level', fallback='INFO').upper()
    log_file = config.get('logging', 'log_file_path', fallback='logs/scraper.log')
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    logger = logging.getLogger("slapdotred_scraper")
    logger.setLevel(log_level)
    
    if logger.hasHandlers():
        logger.handlers.clear()

    fh = RotatingFileHandler(log_file, maxBytes=1*1024*1024, backupCount=5)
    formatter = DetailFormatter('%(asctime)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s')
    fh.setFormatter(formatter)
    logger.addHandler(fh)
    return loggerimport aiohttp, logging, asyncio
from typing import Optional, List, Dict, Any
from models import AuthData

async def get_bonuses(auth: AuthData, session: aiohttp.ClientSession, logger: logging.Logger) -> Optional[List[Dict[str, Any]]]:
    payload = {"module": "/users/syncData", "merchantId": auth.merchant_id, "accessId": auth.access_id, "accessToken": auth.token}
    try:
        async with session.post(auth.api_url, data=payload, proxy=None, timeout=15, ssl=False) as response:
            response.raise_for_status()
            res_json = await response.json()
            if res_json.get("status") != "SUCCESS":
                logger.warning("bonus_api_status_fail", extra={"url": auth.api_url, "response": res_json})
                return None
            bonus_l = res_json.get("data", {}).get("bonus", [])
            promo_l = res_json.get("data", {}).get("promotions", [])
            return (bonus_l if isinstance(bonus_l, list) else []) + (promo_l if isinstance(promo_l, list) else [])
    except Exception as e:
        logger.error("bonus_fetch_fail", extra={"url": auth.api_url, "err": str(e)})
        return Noneimport json
import logging
from typing import Any, List, Dict

from models import Bonus

def _parse_float(value: Any) -> float:
    if value is None: return 0.0
    try:
        return float(value)
    except (ValueError, TypeError):
        return 0.0

def _create_and_map_bonus(data: Dict[str, Any], url: str, merchant_name: str) -> Bonus:
    b = Bonus()
    b.url = url
    b.merchant_name = merchant_name
    b.id = str(data.get("id", ""))
    b.name = str(data.get("name", ""))
    b.amount = _parse_float(data.get("amount"))
    b.rollover = _parse_float(data.get("rollover"))
    b.bonus_fixed = _parse_float(data.get("bonusFixed"))
    b.min_withdraw = _parse_float(data.get("minWithdraw"))
    b.max_withdraw = _parse_float(data.get("maxWithdraw"))
    b.min_topup = _parse_float(data.get("minTopup"))
    b.max_topup = _parse_float(data.get("maxTopup"))
    b.withdraw_to_bonus_ratio = b.min_withdraw / b.bonus_fixed if b.bonus_fixed != 0 else None
    b.transaction_type = str(data.get("transactionType", ""))
    b.balance = str(data.get("balance", ""))
    b.bonus = str(data.get("bonus", ""))
    b.bonus_random = str(data.get("bonusRandom", ""))
    b.reset = str(data.get("reset", ""))
    b.refer_link = str(data.get("referLink", ""))
    b.raw_claim_config = data.get("claimConfig", "")
    b.raw_claim_condition = data.get("claimCondition", "")
    return b

def _parse_claim_config(b: Bonus, logger: logging.Logger) -> Bonus:
    raw_config = b.raw_claim_config
    if not isinstance(raw_config, str) or not raw_config.startswith('['):
        return b
    try:
        config_list = json.loads(raw_config)
        if not isinstance(config_list, list): return b
        for item in config_list:
            if not isinstance(item, str): continue
            iu = item.upper()
            if "AUTO_CLAIM" in iu: b.is_auto_claim = True
            if "VIP" in iu: b.is_vip_only = True
            if "DEPOSIT" in iu: b.claim_type = "DEPOSIT"
            if "RESCUE" in iu: b.claim_type = "RESCUE"
            if "REBATE" in iu: b.claim_type = "REBATE"
            if "LOSS" in iu:
                b.has_loss_requirement = True
                parts = item.split('_')
                if len(parts) > 1:
                    val_str = parts[-1].replace('%', '')
                    if '%' in parts[-1]: b.loss_req_percent = _parse_float(val_str)
                    else: b.loss_req_amount = _parse_float(val_str)
            if "TOPUP" in iu:
                b.has_topup_requirement = True
                parts = item.split('_')
                if len(parts) > 1: b.topup_req_amount = _parse_float(p[-1])
    except Exception as e:
        logger.debug("claim_config_parse_fail", extra={"id": b.id, "err": str(e)})
    return b

def process_bonuses(bonuses_json: List[Dict[str, Any]], url: str, merchant_name: str, logger: logging.Logger) -> List[Bonus]:
    """
    Takes a list of raw bonus dictionaries from the API and returns a list
    of fully processed Bonus objects.
    """
    processed_list = []
    for bonus_data in bonuses_json:
        if not isinstance(bonus_data, dict):
            continue
        bonus_obj = _create_and_map_bonus(bonus_data, url, merchant_name)
        fully_processed_bonus = _parse_claim_config(bonus_obj, logger)
        processed_list.append(fully_processed_bonus)
    return processed_listimport os
import csv
import logging
from typing import List

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.exc import SQLAlchemyError

from models import Bonus, Base

def load_urls(file_path: str, logger: logging.Logger) -> List[str]:
    """Loads URLs from a text file."""
    if not os.path.exists(file_path):
        logger.error(f"URL file not found: {file_path}")
        return []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return [line.strip() for line in f if line.strip() and not line.strip().startswith("#")]
    except Exception as e:
        logger.error(f"Failed to read URL file: {e}")
        return []

def write_bonuses_to_csv(bonuses: List[Bonus], csv_path: str, logger: logging.Logger):
    """
    Correctly serializes detached SQLAlchemy objects and writes them to a CSV file.
    """
    if not bonuses:
        logger.info("No bonuses to write to CSV.")
        return

    try:
        os.makedirs(os.path.dirname(csv_path), exist_ok=True)
        
        # Get headers from the SQLAlchemy model's table columns
        field_names = [c.name for c in Bonus.__table__.columns]
        file_exists = os.path.isfile(csv_path) and os.path.getsize(csv_path) > 0
        
        # Create a list of clean dictionaries from the detached objects
        rows_to_write = []
        for bonus in bonuses:
            row_dict = bonus.__dict__.copy()
            # Remove the internal SQLAlchemy state key before writing
            row_dict.pop('_sa_instance_state', None)
            rows_to_write.append(row_dict)

        with open(csv_path, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=field_names, extrasaction='ignore')
            if not file_exists:
                writer.writeheader()
            writer.writerows(rows_to_write)
        
        logger.info(f"Successfully wrote {len(bonuses)} bonuses to {csv_path}")

    except Exception as e:
        logger.error(f"CSV write failed: {e}")

def write_bonuses_to_db(bonuses: List[Bonus], db_url: str, logger: logging.Logger):
    """Writes a list of Bonus model objects to the database."""
    if not bonuses:
        logger.info("No bonuses to write to database.")
        return
    
    try:
        engine = create_engine(db_url)
        Base.metadata.create_all(engine)
        Session = sessionmaker(bind=engine)
        session = Session()
    except Exception as e:
        logger.error(f"DB engine/session creation failed: {e}")
        return

    try:
        # Use merge to prevent errors with objects that might already exist
        # in the session from a different context.
        for bonus in bonuses:
            session.merge(bonus)
        session.commit()
        logger.info(f"Wrote {len(bonuses)} bonuses to database.")
    except SQLAlchemyError as e:
        logger.error(f"DB write failed: {e}")
        session.rollback()
    finally:
        session.close()

# Dummy cache functions for compatibility
def load_run_cache(logger: logging.Logger): return {}
def save_run_cache(data: dict, logger: logging.Logger): passimport sys
import time
from typing import Deque

class UIHandler:
    def __init__(self):
        self.total = 0
        self.processed = 0
        self.errors = 0
        self.bonuses = 0

    def set_total_urls(self, total: int):
        self.total = total
        if total > 0:
            print(f"Starting scrape of {total} URLs...")

    def update(self, url: str, success: bool, count: int, tracker: Deque[float]):
        self.processed += 1
        self.bonuses += count
        if not success:
            self.errors += 1

        if not sys.stdout.isatty() or self.total == 0:
            return

        # Print a simple, single line for each update
        status = "SUCCESS" if success else "FAIL"
        progress = f"[{self.processed}/{self.total}]"
        print(f"{progress:<12} {status:<8} | Bonuses: {count:<4} | URL: {url}")


    def final(self, found: int, failed: int):
        print(f"\n{'='*40}\nScraping Complete")
        print(f"Total Bonuses Found: {found}")
        print(f"Failed URLs: {failed}")
        print("="*40)import asyncio
import aiohttp
import logging
import configparser
import random
import time
import collections
from typing import List, Deque
from urllib.parse import urlparse, urlunparse

import io_handler, ui, processing, auth, models, config, logger_config, api_client

async def process_url(url: str, app_config: configparser.ConfigParser, logger: logging.Logger, session: aiohttp.ClientSession, request_tracker: Deque[float]):
    """
    Processes a single URL and returns a tuple with all necessary results.
    """
    cleaned_url = urlunparse(urlparse(url)._replace(path="", params="", query="", fragment=""))
    
    auth_data = await auth.get_auth(cleaned_url, app_config, logger, session, request_tracker)
    if not auth_data:
        return [], cleaned_url, False, 0

    min_delay = app_config.getfloat('scraper', 'min_request_delay', fallback=1.0)
    max_delay = app_config.getfloat('scraper', 'max_request_delay', fallback=3.0)
    await asyncio.sleep(random.uniform(min_delay, max_delay))

    bonuses_json = await api_client.get_bonuses(auth_data, session, logger)
    if bonuses_json is None:
        return [], cleaned_url, True, 0

    processed_bonuses = processing.process_bonuses(bonuses_json, cleaned_url, auth_data.merchant_name, logger)
    bonus_count = len(processed_bonuses)
    logger.info(f"OK: {cleaned_url} - Found {bonus_count} bonuses.")
    return processed_bonuses, cleaned_url, True, bonus_count

async def main():
    app_config = config.get_config()
    logger = logger_config.setup_logger(app_config)
    
    urls = io_handler.load_urls(app_config.get('scraper', 'url_list_path'), logger)
    
    ui_handler = ui.UIHandler()
    ui_handler.set_total_urls(len(urls))
    
    if not urls:
        return
        
    request_tracker: Deque[float] = collections.deque(maxlen=200)
    total_bonuses_found = 0
    failed_url_count = 0
    
    # Pre-get output settings
    db_enabled = app_config.getboolean('output', 'enable_db_output')
    csv_enabled = app_config.getboolean('output', 'enable_csv_output')
    db_url = app_config.get('output', 'db_connection_string')
    csv_path = app_config.get('output', 'csv_output_path')
    
    async with aiohttp.ClientSession() as session:
        for url in urls:
            try:
                bonuses_list, cleaned_url, success, bonuses_found = await process_url(url.strip(), app_config, logger, session, request_tracker)
                
                if not success:
                    failed_url_count += 1
                
                if bonuses_list:
                    total_bonuses_found += len(bonuses_list)
                    
                    # --- Real-time Output Logic ---
                    if db_enabled:
                        io_handler.write_bonuses_to_db(bonuses_list, db_url, logger)
                    if csv_enabled:
                        io_handler.write_bonuses_to_csv(bonuses_list, csv_path, logger)
                
                ui_handler.update(cleaned_url, success, bonuses_found, request_tracker)

            except Exception as e:
                failed_url_count += 1
                cleaned_url = urlunparse(urlparse(url.strip())._replace(path="", params="", query="", fragment=""))
                ui_handler.update(cleaned_url, False, 0, request_tracker)
                logger.error(f"A task failed for URL {url.strip()}: {e}", extra={"err":str(e)})

    # Final summary printout
    ui_handler.final(total_bonuses_found, failed_url_count)
    logger.info(f"Scraping complete.", extra={"total_bonuses_found": total_bonuses_found, "failed_urls": failed_url_count})

if __name__ == "__main__":
    asyncio.run(main())# captcha_urls_android.py
# A simple script to bulk-open a list of URLs on Android using Termux.
# This list is populated with sites that failed due to an "Invalid Captcha" error.

import os
import time

# --- URL List for 'Invalid Captcha' Failures ---
urls_to_open = [
    "https://100pokies.com",
    "https://9aus.com",
    "https://afl88.com",
    "https://all117.com",
    "https://anzspin.com",
    "https://ariswin.com",
    "https://arkspin.com",
    "https://aus2u.com",
    "https://aus33.com",
    "https://ausbet88.com",
    "https://aussie21.com",
    "https://aussieluck33au.com",
    "https://ausking777.com",
    "https://auslot7.com",
    "https://avengers9.net",
    "https://bacca777.com",
    "https://bankau.live",
    "https://bet365aud.com",
    "https://betaaron.com",
    "https://betblaze.org",
    "https://betcody.com",
    "https://betjohn.net",
    "https://betman9.com",
    "https://betnich.com",
    "https://betoptus.com",
    "https://betus10.co",
    "https://betworld96au.com",
    "https://betzilla88.com",
    "https://bigpay77.net",
    "https://bizzo777.com",
    "https://blackpokies.com",
    "https://blaze007.com",
    "https://bm7au.com",
    "https://bmb99.com",
    "https://bn8aus.com",
    "https://bondi333.com",
    "https://bonsai369.com",
    "https://bonza7.com",
    "https://bonza96.com",
    "https://boombaby9.com",
    "https://boom966.com",
    "https://boss365au.com",
    "https://bountyspin.com",
    "https://bpay7.com",
    "https://breakspin.com",
    "https://breakwin.com",
    "https://brismelb6.co",
    "https://buffalo39a.com",
    "https://bunny96.com",
    "https://bybid9.com",
    "https://bx77au.com",
    "https://candy96.com",
    "https://cashking99.com",
    "https://cergas.online",
    "https://champion9.com",
    "https://checkmate7.com",
    "https://class777.com",
    "https://click96.com",
    "https://clownwin.com",
    "https://cocainespin.com",
    "https://cocspin.com",
    "https://cola88au.co",
    "https://coospin.com",
    "https://crown69.com",
    "https://crown69.co",
    "https://crown777au.com",
    "https://crownbet.pro",
    "https://crystalchips777.com",
    "https://cuntspin.com",
    "https://cuntwin.com",
    "https://cyberpunk369.com",
    "https://dd8au.com",
    "https://dnf33.com",
    "https://dogdog11.com",
    "https://dolphin88.co",
    "https://donaldwin.com",
    "https://dowin8aus.com",
    "https://drpokies.com",
    "https://dsyaus.com",
    "https://e99au.com",
    "https://ecstasybaby.com",
    "https://emax7.co",
    "https://emu668.co",
    "https://enjoy007.com",
    "https://enjoy2win99.com",
    "https://enjoy33.vip",
    "https://epicpokies.com",
    "https://eq9au.com",
    "https://everwin44.com",
    "https://ex77au.com",
    "https://extrawin9.com",
]
# --------------------------------------------------

# Main execution loop
for url in urls_to_open:
    # Construct the command using the termux-open utility
    command = f"termux-open '{url}'"
    
    print(f"Opening: {url}")
    
    # Execute the command
    os.system(command)
    
    # Wait for 2 seconds before opening the next link. Adjust if needed.
    time.sleep(2)

print("\n--- All links have been opened. ---")# tests/test_processing.py
import pytest
from processing import process_claim_config
from models import BonusDB

def test_process_claim_config():
    bonus = Bonus(url="http://example.com", merchant_name="Test", id="1", name="Bonus", amount=100.0, rollover=30, is_active=True, is_completed=False, is_sticky=None, activated_date_time=None, expiry_date_time=None, claim_config='{"auto_claim": true, "group_id": 1, "wagering": {"requirement": "30x"}}')
    logger = logging.getLogger("test")
    result = process_claim_config(bonus, logger)
    assert result.is_auto_claim is True
    assert result.is_vip_only is True
    assert result.wagering_requirement == 30.0
<<<<<<< HEAD
[auth]
username = 61423349819 ; Required: Login username/mobile number.
password = Falcon66! ; Required: Login password.

[scraper]
url_list_path = urls.txt ; Required: Path to the list of target URLs.
max_concurrent_requests = 1 ; Number of parallel tasks.
min_request_delay = 0.6 ; Minimum seconds between requests.
max_request_delay = 1.4 ; Maximum seconds between requests.

[output]
enable_csv_output = true
csv_output_path = data/bonuses.csv
enable_db_output = true
db_connection_string = sqlite:///data/bonuses.db

[logging]
log_level = DEBUG ; Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
log_file_path = log/log.log
=======
[credentials]
mobile = 61423349819
password = Falcon66!

[settings]
file = urls.txt
downline = False 
# Set to True to scrape downlines, False for bonuses

[logging]
log_file = logs/cli_scrape.log
log_level = INFO 
# DEBUG, INFO, WARNING, ERROR, CRITICAL
console = True   
# True to also log to console (summary format)
detail = MORE    
# LESS, MORE, MAX (controls verbosity of file logs for specific events)

; You can keep the [google_sheets] section if present; the script will ignore it.
>>>>>>> 7e18f1ccb6843a7fa0ccd1b60c2fd7d451dd0f41
